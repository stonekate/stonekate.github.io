[{
    "title": "For participants",
    "date": "",
    "description": "",
    "body": " UoH Psycholinguistics Lab studies currently recruiting participants: Speed and accuracy in processing sentences\nStudienergebnisse für Teilnehmende an der Universität Potsdam Klicke hier für Gurdibu und Budalang EEG\nKlicke hier für ITSAR EEG\nErgänzende Informationen zum Datenschutz [pdf herunterladen]\n",
    "ref": "/forparticipants/"
  },{
    "title": "Tutorial: An RSVP experiment in OpenSesame",
    "date": "",
    "description": "",
    "body": "\rOpenSesame is a free, open source tool for building experiments. In this tutorial, I build a rapid serial visual presentation (RSVP) paradigm of the kind typically used for presenting sentences word-by-word to participants in ERP experiments. The paradigm will send triggers at particular words to the EEG recording computer so that these words can later be located in the EEG recording.\nYou will need to have OpenSesame installed: https://osdoc.cogsci.nl. There are also several tutorials on that page. The code used in the following tutorial is a mix of sophisticated stuff from this attentional-blink tutorial and Bruno Nicenboim. Anything clunky and any mistakes are my additions.\nThe code below was made with OpenSesame v3 and has been updated to work with v4+. But do let me know if anything doesn’t work!\nExperiment structure in OpenSesame\rAn OpenSesame experiment contains sequences , which determine the order that different parts of the experiment are shown. Within a sequence, you can have loops , which show individual trials in a loop until some stopping point is reached. For the experiment in this tutorial, an individual trial is one sentence. The stopping point will be when all sentences have been shown. Here I’ve made a practice loop and an experimental loop. Some other useful objects are sketchpads which we can use to display participant instructions and an object where we can write our own Python code :\n\r\r\rGetting started\rThe steps below will re-create the experiment structure above. First, open a new OpenSesame experiment and click on New experiment in the Overview pane. This will open a tab in the main area called General properties. In General properties, you can change the name New experiment to something more interesting; I’ve called mine Gurdibu. I use xpyriment for the Back-end, but on computers with limited graphics capabilities, you might have to choose legacy. This can be changed any time without breaking the experiment. The screen resolution should be that of your lab presentation PC. You can adjust the default font size/colour etc. to suit your lab.\n\r\rNext, delete any default objects that are currently sitting under experiment.\nVery important: Save your experiment now, and keep saving it throughout.\n\rStep 1: Add a test mode\rIn later steps, we’ll send triggers to the EEG recording computer via the parallel port. Read more about parallel ports in OpenSesame here. But if you’re using a computer that doesn’t have a parallel port (i.e. pretty much any computer that’s not in your EEG lab), OpenSesame will throw an error if you try to send a trigger. To be able to test our paradigm without a port, we can create a test mode where triggers are simply printed in OpenSesame’s code console rather than sent to the port.\nFrom the set of objects on the left, drag and drop an inline_script item onto the experiment sequence. It should appear one level below experiment, as in the first screenshot above. Right click on the inline script item and rename it settings. Here we will put some general settings and functions we need for the experiment, starting with our test mode.\nPython inline scripts have two elements: a prepare phase and a run phase. You can read about the difference between these two phases here.\nFor our test mode, we need to paste the following code into the Prepare tab:\n# import the python libraries we will use\rfrom psychopy import parallel\rimport math\rimport csv\r# set test mode to yes or no var.test_mode = \u0026quot;yes\u0026quot; # test mode function\r# if we are not in test mode...\rif(var.test_mode != \u0026quot;yes\u0026quot;):\r# this address will be different for your port:\rport = parallel.ParallelPort(address=\u0026#39;0x3FD8\u0026#39;) port.setData(4)\rport.readPin(2)\rport.setPin(2,1)\r# send triggers to the port\rdef sendPP(trigger):\rport.setData(trigger)\rreturn;\r# otherwise, just print triggers else:\rdef sendPP(trigger):\rprint(trigger)\rreturn;\rThen paste the following into the Run tab to give you a warning when you’re in test mode:\nif(var.test_mode == \u0026quot;yes\u0026quot;):\rtest_canvas = Canvas(color = \u0026#39;red\u0026#39;,font_size = 38, font_family = \u0026#39;sans\u0026#39;)\r# Fill it with the word:\rtest_canvas.text(\u0026quot;The experiment is running in test mode.\u0026lt;br /\u0026gt;\u0026lt;br /\u0026gt;No triggers will be sent. \u0026lt;br /\u0026gt;\u0026lt;br /\u0026gt; (Change test_mode to \u0026#39;n\u0026#39; in settings for sending triggers through the parallel port.\u0026quot;)\rtest_canvas.show()\rmy_keyboard = Keyboard()\rkey, end_time = my_keyboard.get_key()\r\rStep 2: Add participant instructions\rNext, from the set of objects on the left, drag and drop a sketchpad into your experiment sequence. The sketchpad should appear at the same level as settings. Right click on the sketchpad and rename it instructions. I have two pages of instructions in my example above, but you can have as many or as few as you need.\nTo write on the sketchpad, in the main pane, click on the text icon and then click in the middle of the sketchpad:\n\r\rType in your instructions and click OK. Above the grid, you can change the appearance of the instructions and decide how long to present the instructions for. Since we would like participants to proceed to the next screen only when they are ready, we will leave the default keypress in the Duration field instead of setting a fixed time.\n\rStep 3: Create a practice loop\rNow we’ll make a loop of practice sentences. Drag and drop a loop so that it’s at the same level as the instructions and settings items. You can drag items up and down in the overview pane to change their order. Right click the loop and rename it practice_loop.\nNext, we need to tell OpenSesame what order we want things to happen in for each practice trial, so drag and drop a sequence onto the loop. You will get a prompt; select Insert into practice_loop:\n\r\rRight click on your new sequence and rename it practice_trial. In the end, our practice trial will look like this:\n\r\rLet’s add each object one by one:\nAt the start of each sentence, we want to make sure any responses from the participant from previous trials are reset to their default states (e.g. previous answers are cleared). To do this, drag and drop a reset_feedback item into practice_trial (not after).\nNext, we want to present a fixation cross to focus participants’ eyes before the first word is presented. Use a sketchpad again (rename it fixation_cross), but this time use the fixdot element to make a dot in the middle of the sketchpad. Change the Duration from keypress to whatever you need (I’ve used 495 ms). Note that OpenSesame recommends to use durations that are 5 ms less than what you actually want to account for screen refresh rate:\n\r\rNext, we want to send a trigger to the recording software to indicate when participants saw the fixation cross. To do this, add an inline script . Call it fixation_trigger. To send a trigger, add the following code to the Run phase:\nsendPP(194)\rclock.sleep(var.trigger_interval)\rsendPP(0)\rTranslated, this code means: send trigger 194, wait a pre-specified interval of time, then send a zero trigger to reset the port. You need the time interval between the 194 and 0 triggers because otherwise they’ll get sent at the same time and you’ll only be able to see the zero in your EEG recording.\nThe pre-specified interval of time should be added to the Prepare tab of the settings inline script (just paste it anywhere in the Prepare tab, the order doesn’t matter). I’ve set it as 45 ms, but it can be anything:\n# minimum interval to wait for trigger\rvar.trigger_interval = 45\rNext, add a jitter screen (call it jitter). This just presents a blank screen for a different, randomly selected duration before each sentence so that participants don’t get entrained to the rhythm of the experiment. I set Duration to 1000 ms with a jitter of 100 ms and jitter mode of standard deviation:\n\r\rThen, we add another inline script called trigger_jitter item to send another trigger:\nsendPP(195)\rclock.sleep(var.trigger_interval)\rsendPP(0)\rThen, add another inline script and call it RSVP. This is where we’ll program the sentences to be presented word-by-word, but we’ll leave that until Step 5.\nAfter RSVP, add another trigger called trigger_endsentence that marks when the sentence finished:\nsendPP(200)\rclock.sleep(var.trigger_interval)\rsendPP(0)\rAfter some sentences I want to ask participants a comprehension question. To present the question, insert a sketchpad. Add some text to the sketchpad as before; here is an example:\n\r\rOpenSesame knows that whatever is written between the square brackets (here: [question]) is a variable that exists in our experimental stimuli. I’ll go into detail about adding stimuli in Step 4, but in general, you would have a table that has columns such as item, condition, sentence, question, correct_response. OpenSesame will automatically pull out the contents of the column question that corresponds to the current sentence. Everything on the sketchpad that is not between square brackets will stay the same on every trial, e.g. here I have a prompt to remind participants which keyboard key corresponds to Yes/No. Set the Duration of this sketchpad to zero: the response object coming up will take care of the sketchpad presentation duration.\nNext, add a trigger called trigger_question to indicate when the question was asked:\nsendPP(196)\rclock.sleep(var.trigger_interval)\rsendPP(0)\rNow we need to enable participants to answer the question by inserting a keyboard response object (there are also mouse and joystick objects if that’s what your lab uses). In the response object, we can leave Correct response blank to use the correct_response from our stimuli table (remember to make sure this is what your correct response column is called!). Fill in the allowed responses and whether you want a timeout; my settings are here:\n\r\rAdd another trigger called trigger_response to signal the response:\nsendPP(197)\rclock.sleep(var.trigger_interval)\rsendPP(0)\rNext, we need to log the response using a logger item . OpenSesame recommends logging all variables; this will include all the columns in your stimuli table, plus a lot of other background things that OpenSesame tracks like timestamps for every step in your trial/loop. These will be saved in a .csv file as the experiment progresses.\nNext, we insert a sketchpad item to offer participants a break. You can set the Duration to a specific time to force participants to take a break of a particular length, or you can let them decide by setting it to keypress:\n\r\rThen of course add a trigger called trigger_break to indicate the break:\nsendPP(198)\rclock.sleep(var.trigger_interval)\rsendPP(0)\rSince we don’t want participants to take a break after every sentence, we can tell OpenSesame to only present this object when some criterion is reached. To set a criterion, go back up and click on practice_trial. In the main window, you will see a list of all the items that appear in that trial, alongside a column called Run if:\n\r\rNext to the break object is where we write our Run if criterion: here I have said that if [live_row_practice_loop] divided by 29 leaves a remainder of 28, then show the break screen. [live_row_practice_loop] is a counter that OpenSesame keeps in the background of how many times the trial loop has been presented (i.e. how many sentences have been presented). So participants in this study get a break every 29 sentences. This number and the remainder of 28 was determined by the number of sentences in my experiment and what they were divisible by, so of course you can set this to whatever you want.\nNote that we have to apply the same rule to the trigger_break so that it is only sent whenever the break occurs.\nI’ve also set some other Run if rules in the practice_loop, namely that the question screen and associated response and trigger items are only shown if there is no NA in the question column of the stimuli table (not all of my sentences have questions).\nThat’s the practice phase done!\n\rStep 4: Adding stimuli\rSo where do the sentences come from? Click on practice_loop in your experiment overview. You have two options here: either you can import stimuli from an external file:\n\r\ror you can write them directly into table provided:\n\r\rWriting them in the table can be a good idea if you’re using non-Roman characters. For example, I’ve had trouble getting OpenSesame to display German words from external files and it took a lot of vague Python error messages to discover that umlauts were the problem (tip: setting UTF-8 encoding via Microsoft Excel or Notepad++ doesn’t work, but UTF-8 encoding from R does! ¯_(ツ)_/¯ ).\nMake sure that the column names of your experimental table or file correspond exactly to the names of these variables that you call elsewhere in the experiment, e.g. on the question sketchpad or in Python inline scripts.\nFor this tutorial, we’ll use the table. To match up with the code for RSVP that you’re going to write below, your table should have the same columns as this:\n\r\r\rStep 5: Setting up rapid serial visual presentation (RSVP)\rWe first need to go back to our settings inline script and add some functions to chop up sentences into words, calculate their presentation durations, and decide for which words to send triggers and what triggers.\nI use a variable presentation duration for each word depending on the word’s length, except for the target word, which I present for 700 ms. But you could also just set each word to appear for the same duration.\nFor my triggers, I base them on my experimental conditions, which I have numbered from 0 to 3 (note that Python starts counting at zero). So that I can later identify which triggers belong to which condition, I multiply the trigger by the condition number. For the target word, I send a different (also condition-based) trigger: this makes it easier to extract target words from the EEG when you have targets in different sentence positions between conditions or items. The target word trigger is sent whenever the word number matches the column target_wordno in the stimuli table. Since Python starts counting words from zero, target_wordno should be 1 less than the actual word number.\nI also only send triggers for words that I might want to analyse later, so I’ve split my sentences into two parts: context and sentence (see the table in Step 4). The code below will send triggers only for the sentence words. But of course you could just have your whole sentence under sentence and send a trigger for each word.\nIn the Prepare tab of settings, add the following code:\n# number of lists (change as appropriate)\rN_lists = 1 # number of sentences\rN_items = 4 # set some presentation durations\rmin_dur = 250 # minimum word presentation duration\rbase_dur = 190 # baseline word presentation duration\rchar_dur = 20 # number of milliseconds to add for each letter in word\rrefresh_red = 5 # number of milliseconds to adjust for screen refresh rate\r# minimum interval to wait for trigger\rvar.trigger_interval = 45\r# function for calculating word presentation duration\rdef pres_dur ( stimulus ):\rreturn max(min_dur, base_dur * len(stimulus.split()) + len(stimulus) * char_dur) - refresh_red\r# function to split up sentences into words and add to canvas, define triggers,\r# and add word presentation durations. I have sections of the sentence where I # don\u0026#39;t need triggers, but other sections where I do want triggers. So the default\r# behaviour of the function is to not send a trigger unless some condition is met.\r# The result of this function is a list of canvases, one for every word in the # sentence, plus a list of presentation durations and triggers for each word.\rdef stim_prep(stims, trigger = \u0026quot;n\u0026quot;):\r# split the sentence by spaces\rstims = stims.split()\r# for every word in the sentence\rfor i, s in enumerate(stims):\r# if we want a trigger (set elsewhere)\rif(trigger == \u0026quot;y\u0026quot;):\r# and if the current word number equals the target word number (from stimuli table)\rif(i == var.target_wordno):\r# send a trigger that is the current word number + the condition # number (from stimuli table) and some other stuff\rsend = (i + (var.cond + 1) * 10) + 50\r# if it\u0026#39;s not the target word, send the trigger like so\relse:\rsend = i + (var.cond + 1) * 10\r# if we don\u0026#39;t want a trigger, send zero\relse:\rsend = 0\r# add this information to a variable called stimuli (defined in RSVP)\rstimuli.append(s)\r# open a blank canvas\rword_canvas = canvas()\r# add the current word to the canvas\rword_canvas.text(s)\r# add this canvas to a list of canvases (the sentence)\rword_canvas_list.append(word_canvas)\r# set the presentation duration of the current word using the pres_dur function\rpres_dur.append(pres_dur( s ))\r# add the trigger for the current word to the list of triggers for the sentence\rtrigger.append(send) return;\rIn the Run tab of settings, add a zero trigger to make sure the parallel port is set to zero at the start of the experiment:\nsendPP(0)\rNext, go to the RSVP Python item in practice_trial. In the Prepare tab of RSVP, use the stim_prep function we defined in settings to chop up the sentence for the current trial into words and compute a presentation duration and trigger number for each word. Paste the following code into Prepare:\n# Here we create some empty lists for storing things (triggers etc.) that will\r# be created by the stim_prep function we defined in settings:\r# list of presentation durations for each stimulus presented\rpres_dur = []\r# list of triggers for each word\rtrigger = []\r# list of words presented\rstimuli = []\r# blank canvas blank_canvas = canvas()\r# list of canvases for creating visual display\rword_canvas_list = []\r# apply stim_prep function to the part of the sentence that doesn\u0026#39;t need triggers\rstim_prep(var.context)\r# apply stim_prep function to the part of the sentence that does need triggers\rstim_prep(var.sentence, trigger = \u0026quot;y\u0026quot;)\r# inter-stimulus interval\rvar.isi = 295\rThe Run tab of RSVP is where the action happens: all of our prepared words and their associated properties (duration, trigger) will now be presented one by one using a loop:\n# for each word in our list of canvases\rfor word_nr, word_canvas in enumerate(word_canvas_list):\r# show the word\rword_canvas.show()\r# send the trigger that corresponds to that word\rtr = var.trigger[word_nr]\rsendPP(tr)\r# if the current word number equals the target word number in the stimuli table,\r# presentation duration should be 700 ms (minus 5 ms for screen refresh rate)\rif word_nr == var.target_wordno: clock.sleep(695)\r# otherwise present it for the corresponding duration from our pres_dur list\relse:\rclock.sleep(pres_dur[word_nr])\r# clear the canvas\rblank_canvas.show()\r# reset the parallel port to zero\rsendPP(0)\r# wait for the length of the inter-stimulus interval before the next word\rclock.sleep(var.isi)\r# Note: As of V4, OpenSesame won\u0026#39;t log variables defined in an `inline_script`\r# unless they are numbers, strings, or None values, even if you prepend them with # `var.`. There are good reasons for this # (see https://osdoc.cogsci.nl/4.0/notes/400/#important-backwards-incompatible-changes).\r# But we want to save our lists of triggers and words etc., which at the moment\r# are list objects, even though they contain strings and numbers. We can convert # them to strings separated by `_` and log them this way instead:\rvar.pres_durs_log = \u0026quot;_\u0026quot;.join(str(i) for i in pres_dur) var.trigger_log = \u0026quot;_\u0026quot;.join(str(i) for i in trigger)\rvar.stimuli_log = \u0026quot;_\u0026quot;.join(stimuli)\rOk, time to test it out! First save your experiment, then press this play button to see if your experiment runs. Don’t confuse the play button with this button: ; I forget what it does, just don’t press it.\n\rStep 7: Set up the experimental loop\rNow we’ve finished our practice phase, we add the real experimental phase. First drag and drop a new loop item into the experiment sequence so that it is at the same level as practice_loop. Rename it experimental_loop. Then, drag and drop a new sequence into experimental_loop and rename it experimental_trial. Copy and paste each of the objects in practice_trial into experimental_trial. To do this, right click on the object under practice_trial (e.g. new_reset_feedback) and select Copy (linked). Now right click on experimental_trial and select paste, and then Insert into experimental_trial. Repeat for all the objects under practice_trial.\nThe benefit of using linked copies is that any edits you make to one object in one loop (e.g. RSVP) will automatically be applied to all linked items in other loops. Moreover, linking helps OpenSesame streamline the logging process leading to faster run-time.\nNext, you need to fill in the table in experimental_loop with your stimuli as in Step 5. You’ll also need to add the break criterion again in experimental_trial, plus the instruction not to present a question if there’s an NA in the question column.\nAt this point you might want to go back to the practice_loop and change the condition numbers in the stimuli table to something that is different to your experimental conditions, otherwise later you won’t be able to tell which triggers in the EEG are for practice trials. You could also add a practice column to your stimuli with “yes/no”, but this will only be saved in the OpenSesame log file and won’t be visible in the EEG.\n\rStep 8: Finishing touches\rFinally, you might like to show participants a message when they finish the practice phase and the experiment. Drag and drop two sketchpads onto the experiment sequence, so that they’re both at the same level as the instructions sketchpad. Drag them so that one is after the practice_loop and one after the experimental_loop. Edit the sketchpads to show participants a message like “That was the practice phase! If you are ready to continue, press the spacebar”.\nI would also recommend sending a trigger at the end of the practice phase called trigger_pracend, after the sketchpad we just added:\nsendPP(189)\rclock.sleep(var.var.trigger_interval)\rsendPP(0)\rand another one called trigger_exp_end for the end of the experiment:\nsendPP(199)\rclock.sleep(var.var.trigger_interval)\rsendPP(0)\rThat’s it, your experiment is now ready!\n\rOptional steps\rLimiting the number of times a condition can be shown consecutively\rThe above experiment will randomise stimulus presentation, but you might want to additionally ensure that stimuli from the same condition are not shown more than, for example, twice in a row. To do this, click on experimental_loop in the file tree. At the top right of the tab that opens, click on this icon and select View script from the dropdown menu that appears. Somewhere in the list, add the following line (I’m not sure if it matters where, but I added it directly before the last line which said run experimental_trial):\n# replace cond with whatever your condition variable is titled:\rconstrain cond maxrep=2\r\rLatin square design\rI mentioned earlier that if you have different experimental lists (e.g. for a Latin square design) and you’re using the OpenSesame table rather than external files for your stimuli, the simplest approach is to have a separate experimental_loop for each list.\nTo do this, create new loops with a trial sequence underneath the current experimental_trial and then Copy (linked) all the trial objects as you did above. Rename your experimental loops with something unique like experimental_loop_list1, experimental_loop_list2, etc. Then you can update the stimuli table with your stimuli from list 2 and so on.\nTo tell OpenSesame which loop to choose, click on the experiment sequence right near the top of the overview pane. We can set a Run if condition to choose a loop depending on subject number:\n\r\rIf you’re using an external source file, you only need one experimental_loop, but separate source files for each list.\nTo call your separate source file lists according to subject number, click on experimental_loop and next to File, enter the condition as I have here:\n\r\rThis condition assumes that each of my stimuli files is called stimuli_1.csv, stimuli_2.csv, etc., and OpenSesame will pick which one based on the subject number. Even fancier would be to have one external stimuli file which OpenSesame loads and splits into lists itself, but I’ll leave that to you.\nHappy experimenting!\n\r\r",
    "ref": "/blog/opensesame/"
  },{
    "title": "Bayesian divergence point analysis of visual world data",
    "date": "",
    "description": "",
    "body": "\r\rThe visual world is a useful paradigm in psycholinguistics for tracking people’s eye fixations as they listen to sentences containing some kind of experimental manipulation. A common question is whether the experimental manipulation makes people look at a target object earlier in one condition than another. To answer this, we need to decide when in each condition people start looking at the target and compare these timepoints between conditions. But this is not as straightforward as it sounds!1 We came up with a bootstrapping method to do this in Stone, Lago \u0026amp; Schad, 2020. However, bootstrapping makes some unlikely assumptions about data—I’ll come to these later—and so here we try to improve on these assumptions by adding Bayesian principles to our method. This was an approach developed with João Veríssimo, Daniel Schad, and Sol Lago, and we apply it in this paper.\nThe bootstrap procedure\rA fully coded tutorial of the bootstrap procedure in Stone et al. (2020) is at https://osf.io/exbmk/. For a quick overview here, we use example data from an experiment on using syntactic gender to predict an upcoming noun in German. Participants heard sentences like “Klicke auf seinen blauen…” (click on his.MASC blue.MASC …), while looking at two blue objects on a screen. Only one of the objects matched the gender marking of the pronoun and adjective. There were two experimental conditions: in the match condition, the target object and the object’s owner matched in gender, e.g. seinen Knopf (his button.MASC). In the mismatch condition, the target object and the object’s owner mismatched in gender, e.g. ihren Knopf (her button.MASC).\n\r\rBecause of the gender cue on the possessive and adjective (e.g. -en suffix), we expected participants to predict the target object and look preferentially at it before they heard its name. What we really wanted to know though was whether predictive looks would be delayed in the mismatch condition where there was a conflicting gender cue, even though the conflicting cue was syntactically irrelevant (i.e. the object’s owner being masculine or feminine has no bearing on what the upcoming object might be). You can already see below that the onset of when participants looked preferentially at the target appears to be later in the mismatch condition:\nBut is this “prediction onset” in the mismatch condition significantly later than the match condition? We can find out using our bootstrapping procedure, which has the following steps:\nConduct a statistical test of fixations between the target and competitor at each timepoint in each condition (similar to a permutation test, e.g. Groppe, Urbach \u0026amp; Kutas, 2011; Maris \u0026amp; Oostenveldt, 2007; Barr, Jackson \u0026amp; Phillips, 2014),\rDecide on an alpha (usually 0.05) and find the first significant test statistic in a run of five consecutive significant test statistics (Sheridan \u0026amp; Reingold, 2012; Reingold \u0026amp; Sheridan, 2014 take a similar approach).2 This was our divergence point for each condition, which we consider the onset of evidence for predictions,\rResample the data 2000 times with replacement and repeat steps 1-2 after each resample.\r\rThe procedure thus has 3 distinct components:\na set of statistical tests comparing fixations to the target vs. competitor,\ra criterion for deciding where the onset is, and\ra way to generate a distribution of these onsets (resampling).\r\rThe unique contribution of our procedure versus existing methods was iii): we estimate the sampling distribution of an onset in each condition, which we can then use to statistically compare onsets between conditions.\nThe procedure yields two bootstrap distributions: one distribution each of onsets for the match and mismatch conditions. We take the mean and the 95th percentile confidence interval (CI) of the match/mismatch distributions as an estimate of the prediction onset for each condition and its temporal variability:\nBy subtracting the match from the mismatch distribution, we obtain a distribution of differences between conditions. We take the mean and the 95th percentile CI of this difference distribution as the estimated difference in prediction onset. We can decide whether the difference is different from zero by looking at whether the 95th percentile CI of the difference distribution contains zero. Since it does not, we can conclude that the difference between conditions is not zero. Moreover, since all values in the distribution are positive, we can conclude that predictions were slower in the mismatch condition:\nLimitations of the bootstrap procedure\rAs mentioned earlier, the bootstrap makes some unlikely assumptions (Bååth, 2018):\n\rAny values not seen in the observed data are impossible\rEach value in the observed data has an equal probability of occurring every time the experiment is run\rWe have no prior knowledge about what datapoints might be observed\r\rThese assumptions limit our interpretation of the results. Another limitation of our procedure above is that while it allows us to conclude that there is a significant difference between match and mismatch prediction onsets, it does not allow us to quantify how much evidence we have for this conclusion.\n\r\rAdding Bayesian principles\rBayesian inference estimates our certainty about an event based on our prior knowledge about the probability of that event and new data. In our case, the event is prediction onsets. We can estimate our certainty about prediction onsets via Bayes’ theorem, which estimates a posterior probability distribution using two components: priors to encode our expectations about when the onset could be, and data to inform posterior inference via a likelihood function3.\nWe start with the priors: we reasoned that prediction onsets could only arise in the 1600 ms time window between the onset of the pronoun and the onset of the noun (adding 200 ms for saccade planning). We therefore specified a normal distribution centered in the middle of this critical window, with a 95% probability of the onset falling between 200 and 1800 ms: \\(N(1000,400)\\):\nprior_mean \u0026lt;- 1000\rprior_sd \u0026lt;- 400\rOur prior distributions for the match and mismatch conditions therefore looked like this:\nNext, we needed a likelihood function. This involved two steps: We used the bootstrap data to approximate a likelihood and a normal distribution to approximate a likelihood function (i.e. Laplace approximation). This was based on the assumption of the central limit theorem that the population distribution underlying the bootstrap data was approximately normal4. In other words, our likelihood function was a normal distribution with the mean and standard deviation of the bootstrap data:\n# match condition\rlikelihood_mean_match \u0026lt;- mean(bootstrap_samples$match, na.rm = TRUE)\rlikelihood_sd_match \u0026lt;- sd(bootstrap_samples$match, na.rm = TRUE)*\r(length(bootstrap_samples$match)-1)/length(bootstrap_samples$match)\r# mismatch condition\rlikelihood_mean_mismatch \u0026lt;- mean(bootstrap_samples$mismatch, na.rm = TRUE)\rlikelihood_sd_mismatch \u0026lt;- sd(bootstrap_samples$mismatch, na.rm = TRUE)*\r(length(bootstrap_samples$mismatch)-1)/length(bootstrap_samples$mismatch)\rFinally, we could now derive the posterior distribution as the product of the prior and the likelihood. Because the prior and the likelihood are normal distributions, the posteriors for each condition can be derived analytically as the product of two Gaussian probability density functions.\n\r\\(N(\\mu_{prior},\\sigma_{prior})\\) is the prior,\r\\(N(\\mu_{lik},\\sigma_{lik})\\) is the likelihood,\r\\(\\mu_{posterior}\\) is the mean of the posterior, and\r\\(\\sigma_{posterior}\\) is the standard deviation of \\(\\mu_{posterior}\\)\r\rThe posterior distribution of the onset in the match condition is therefore:\n\\[ \\begin{aligned}\r\\mu_{posterior} \u0026amp;= \\frac{\\mu_{prior} \\sigma_{lik}^2 + \\mu_{lik} \\sigma_{prior}^2}{\\sigma_{lik}^2 + \\sigma_{prior}^2} \\\\\r\u0026amp;= \\frac{1000 \\cdot 22^2 + 367 \\cdot 400^2}{22^2 + 400^2} \\\\ \u0026amp;= 369 ms\r\\end{aligned}\r\\]\n\\[ \\begin{aligned}\r\\sigma_{posterior} \u0026amp;= \\sqrt\\frac{\\sigma_{prior}^2 \\sigma_{lik}^2}{\\sigma_{prior}^2 + \\sigma_{lik}^2} \\\\ \u0026amp;= \\sqrt\\frac{400^2 \\cdot 22^2}{400^2 + 22^2} \\\\ \u0026amp;= 22 ms \\end{aligned}\r\\]\rVia the same calculation, the posterior for the mismatch onset is 705 ms (SD = 9 ms):\n# match\rposterior_mean_match \u0026lt;- ((likelihood_mean_match*prior_sd^2) + (prior_mean*likelihood_sd_match^2)) /\r(prior_sd^2 + likelihood_sd_match^2)\rposterior_sd_match \u0026lt;- sqrt( (likelihood_sd_match^2 * prior_sd^2) / (likelihood_sd_match^2 + prior_sd^2) )\r# mismatch\rposterior_mean_mismatch \u0026lt;- ((likelihood_mean_mismatch*prior_sd^2) + (prior_mean*likelihood_sd_mismatch^2)) /\r(prior_sd^2 + likelihood_sd_mismatch^2)\rposterior_sd_mismatch \u0026lt;- sqrt( (likelihood_sd_mismatch^2 * prior_sd^2) / (likelihood_sd_mismatch^2 + prior_sd^2) )\rWe can add these posteriors to our plots and see that they resemble the bootstrap data:\nThis is because our prior was relatively uninformative, and so the posteriors are informed more strongly by the bootstrap data than by the prior. If we had defined a strongly informative prior to say we were very certain that the prediction onsets would be 1000 ms, e.g. \\(N(1000, 10)\\), then the posteriors would be pulled toward the prior (not completely, as they’re still being informed by the data):\nHow do the Bayesian onsets (red, uninformative priors) compare with the onsets from the original bootstrap procedure (black)? Quite well:\nNow, how to decide whether the mismatch condition was slower? Because the match and mismatch posteriors are normal distributions, we can find the posterior of their difference as the difference of two normal distributions:\n\\[ \\begin{aligned}\r\\mu_{posterior_{difference}} \u0026amp;= \\mu_{posterior_{mismatch}} - \\mu_{posterior_{match}} \\\\\r\u0026amp;= 705 - 369 \\\\\r\u0026amp;= 336 ms\r\\end{aligned}\r\\]\n\\[ \\begin{aligned}\r\\sigma_{posterior_{difference}} \u0026amp;= \\sqrt(\\sigma^2_{posterior_{mismatch}} + \\sigma^2_{posterior_{match}}) \\\\\r\u0026amp;= \\sqrt(9^2 + 22^2) \\\\\r\u0026amp;= 24ms\r\\end{aligned}\r\\]\nposterior_mean_difference \u0026lt;- posterior_mean_mismatch - posterior_mean_match\rposterior_sd_difference \u0026lt;- sqrt( posterior_sd_mismatch^2 + posterior_sd_match^2 )\rWe now have a posterior estimate that predictions in the mismatch condition were 336 ms slower than in the match condition, with a 95% credible interval of 288–384 ms. This posterior aligns quite well with the original bootstrap difference distribution, which estimated a difference of 338 ms with a 95th percentile interval of 300–380ms.\nQuantifying evidence with a Bayes factor\rHow much has seeing the data changed our belief in the null hypothesis that the difference between match/mismatch conditions is zero? Since the likelihood is not normalised, we can use the Savage-Dickey method to compute a Bayes factor and quantify evidence against the null (Dickey \u0026amp; Lientz, 1970; Wagenmakers et al., 2010). This method finds the ratio of prior to posterior density at some point value (e.g. zero):\nWe compute the ratio via the equation below, where\n\r\\(\\theta\\) is the point at which we want to compare densities (e.g. zero),\r\\(H_0\\) is our prior distribution,\r\\(H_1\\) is our posterior distribution,\rand \\(D\\) is the data:\r\r\\[ BF_{01} = \\frac{p(D|H_1)}{p(D|H_0)}= \\frac{p(\\theta = 0|H_1)}{p(\\theta = 0|D,H_1)} \\]\nOr in R form:\n# define the null hypothesis\rnull_hypothesis \u0026lt;- 0\r# define a prior for the distribution of differences prior_mean_difference \u0026lt;- prior_mean - prior_mean\rprior_sd_difference \u0026lt;- sqrt(prior_sd^2 + prior_sd^2)\r# find the density of the prior at the null hypothesis\rdensity_prior_null \u0026lt;- dnorm(null_hypothesis, prior_mean_difference, prior_sd_difference)\r# find the density of the posterior at the null hypothesis\rdensity_posterior_null \u0026lt;- dnorm(null_hypothesis, posterior_mean_difference, posterior_sd_difference)\r# use Savage-Dickey equation to compute the Bayes factor\r(BF01 \u0026lt;- density_posterior_null/density_prior_null)\r## [1] 1.402512e-44\rThe Bayes factor can be interpreted as a ratio of evidence for one hypothesis over the other. Because our posterior density at zero is less than the prior density (in fact, the posterior density at zero is almost zero), the Bayes factor is less than 1 and thus favours the alternative hypothesis that there is a match/mismatch difference. We can see more clearly how overwhelminlgy it favours the alternative hypothesis if we flip the ratio to test the alternative vs. null hypotheses (BF10) rather than the null vs. alternative (BF01):\n(BF10 \u0026lt;- 1/BF01)\r## [1] 7.130063e+43\r\r\rConclusions\rUsing our Bayesian divergence point analysis, we find strong evidence that prediction onsets were slower when there were two conflicting gender cues. The posterior estimate of the “mismatch effect” size was 336 ms, with a 95% credible interval of 288–384 ms. But our existing bootstrap procedure already led us to the same conclusion, so what was the advantage of adding Bayesian principles?\nThe Bayesian approach has all the advantages that Bayesian inference has over frequentist null hypothesis significance testing. These include that the posterior is a continuous probability distribution that gives us information about the probability of non-observed values in the data. The 95% credible interval can thus be interpreted as a range of possible between-condition differences in which the true size of the difference should lie with 95% probability, given the data and the analysis. This interpretation is more intuitive than the percentile confidence interval from the existing bootstrap method, which just told us were 95% of the resampled data lay. Moreover, we can now quantify evidence for our conclusions, and even for the null hypothesis, using a Bayes factor. Finally, because the Bayesian method uses priors, we can use our posterior estimates as information about plausible effect sizes to inform the priors of future experiments.\n\r\rThere are several existing methods for answering temporal questions about visual world data, including cluster permutation, BDOTS, and GAMMs. We summarise these in Stone et al., 2020 and outline why they weren’t able to answer our specific question about whether one onset was significantly faster than another.↩︎\n\rDepending on your experimental manipulation (e.g. how big or sustained you expect your experimental effect to be) and how you’ve set up your data (e.g. binned, unbinned, eye tracker sampling rate), your criterion for the number of consecutive significant tests may differ.↩︎\n\rOther possibilities for adding Bayesian principles to our procedure could have been to use the Bayesian Bootstrap (Rubin, 1981), implemented in R in bayesboot (Bååth, 2018). Unfortunately, bayesboot didn’t suit our particular bootstrapping method, partly because it doesn’t take a multi-step function like ours (see steps 1-3 above), but also because it doesn’t take more informative priors—at least not currently. Alternatively, we could have fit e.g. GLMMs in brms (Bürkner, 2018) to test between fixation proportions at each timepoint (step 1). But this would only apply Bayesian inference to estimating the magnitude of the difference in fixations between target and competitor at each timepoint, when what we really want is to apply it to finding the temporal variability of the onset.↩︎\n\rUsing a normal distribution for the likelihood assumes that, with sufficient observations and bootstrap samples, the bootstrap distribution will approach a normal distribution in line with the central limit theorem. However, this is not always the case. An alternative way to define the likelihood would be to use a kernel density estimator instead: we present this approach in the appendices of this paper.↩︎\n\r\r\r",
    "ref": "/blog/bpda/"
  },{
    "title": "Publications",
    "date": "",
    "description": "",
    "body": " 2025 Stone, K. \u0026amp; Rabovsky, M. (2025) The role of syntactic and semantic cues in resolving illusions of plausibility. Journal of Cognitive Neuroscience. [accepted manuscript] [data/code] [journal page]\nLago, S., Oltrogge, E. \u0026amp; Stone, K. (2025) Does productive agreement morphology increase sensitivity to agreement in a second language? Glossa Psycholinguistics. [paper] [data/code]\n2024 Vanek, N., Matić Škorić, A., Košutar, S., Matĕjka, S. \u0026amp; Stone, K. (2024) Looks at what isn\u0026rsquo;t there: eye movements on a blank screen when processing negation in a first and a second language. Frontiers in Human Neuroscience. [paper] [data/code]\nVanek, N., Matić Škorić, A., Košutar, S., Matĕjka, S. \u0026amp; Stone, K. (2024) Mental simulation of the factual and the illusory in negation processing: evidence from anticipatory eye movements on a blank screen. Scientific Reports. [paper] [data/code]\n2023 Stone, K., Khaleghi, N. \u0026amp; Rabovksy, M. (2023) The N400 is elicited by meaning changes but not synonym substitutions: Evidence from Persian phrasal verbs. Cognitive Science. [paper] [data/code]\nStone, K., Nicenboim, B., Vasishth, S. \u0026amp; Rösler, F. (2023) Understanding the effects of constraint and predictability in ERP. Neurobiology of Language. [paper] [data/code]\n2022 Lago, S., Stone, K., Oltrogge, E. \u0026amp; Veríssimo, J. (2022) Possessive predictions in bilingual comprehension. Language Learning. [paper] [data/code]\nStone, K., Vasishth, S. \u0026amp; von der Malsburg, T. (2022) Does entropy modulate the prediction of German long-distance verb particles? PLOS ONE. [paper] [data/code]\n2021 Stone, K., Veríssimo, J. Schad, D., Oltrogge, E., Vasishth, S. \u0026amp; Lago, S. (2021) The interaction of grammatically distinct agreement dependencies in predictive processing. Language, Cognition and Neuroscience. [paper] [data/code]\n2020 Stone, K., von der Malsburg, T. \u0026amp; Vasishth, S. (2020) The effect of decay and lexical uncertainty on processing long-distance dependencies in reading. PeerJ. [paper] [data/code]\nStone, K., Lago, S. \u0026amp; Schad, D. (2020) Divergence point analyses of visual world data: applications to bilingual research. Bilingualism: Language and Cognition. [paper] [data/code]\n [poster]\r**Stone, K.** \u0026 Rabovsky, M. (2023) Resolving illusions of plausibility: Competition between syntax and semantics over time? Poster presented at HSP, Pittsburgh, March 2023. [poster]\rKhaleghi, N., **Stone, K.**, \u0026 Rabovsky, M. (2022) Semantic versus lexical violations in Persian phrasal verbs: An EEG experiment. Poster presented at AMLaP, York, September 2022.\r**Stone, K.**, Nicenboim, B., Vasishth, S., \u0026 Rösler, F. (2022) Dissociating constraint and predictability in ERP: Evidence from German. Poster presented at HSP in Santa Cruz, March 24, 2022. [poster]\r**Stone, K.**, Vasishth, S., Rösler, F. \u0026 Nicenboim, B. (2021) Maximising ERP resources using a sequential Bayes factor approach to sample size. Short talk presented at AMLaP in Paris, France, September 4, 2021. [slides]\rLago, S., Oltrogge, E., \u0026 **Stone, K.** (2020) Are native and non-native speakers differentially sensitive to agreement attraction? Poster presented at AMLaP in Potsdam, Germany, September 4, 2020. [poster]\r**Stone, K.**, Oltrogge, E., Vasishth, S. \u0026 Lago, S. (2020) The real-time application of grammatical constraints to prediction: Timecourse evidence from eye tracking. Talk presented at CUNY in Amherst, USA, March 20, 2020. [slides]\r**Stone, K.**, Vasishth, S. \u0026 von der Malsburg, T. (2020) Contextual constraint and the frontal post-N400 positivity: A large-sample, pre-registered ERP study. Poster presented at CUNY in Amherst, USA, March 20, 2020. [poster]\r**Stone, K.** \u0026 Lago, S. (2020) Individual variability in the timecourse of predictions. Talk presented at DGfS in Hamburg, Germany, March 4, 2020. [slides]\r**Stone, K.** \u0026 Lago, S. (2019) L1 influence in L2 gender predictions: A visual world study. Poster presented at ESCoP in Tenerife, Spain, September 26, 2019. [poster]\r**Stone, K.**, Vasishth, S. \u0026 von der Malsburg, T. (2019) Comprehenders generate long-distance predictions during reading: ERP evidence from verb particle constructions. Talk presented at PIPP in Reykjavik, Iceland, June 19-20, 2019. [slides]\r**Stone, K.**, Vasishth, S. \u0026 von der Malsburg, T. (2019) ERP evidence for long-distance lexical predictions in German particle verb constructions. Poster presented at CUNY in Boulder, USA, March 29-31, 2019. [poster]\r**Stone, K.**, Vasishth, S. \u0026 von der Malsburg, T. (2018) Expectations and prediction in sentence processing: German particle verbs as a test case. Poster presented at CUNY in Davis, USA, March 15-17, 2018. [poster]\r**Stone, K.**, Mertzen, D. \u0026 Vasishth, S. (2017) Verb particle predictability determines the facilitation effect of pre-particle material. Poster presented at Architectures and Mechanisms of Language Processing (AMLaP) in Lancaster, UK, September 7-9, 2017. [poster]\r**Stone, K.**, Jessen, A. \u0026 Clahsen, H. (2016) Delayed versus immediate production: ERP studies of verb morphology. Poster presented at the International Workshop on Language Production in San Diego, USA, July 25-27, 2016.\r--  ",
    "ref": "/publications/"
  }]
