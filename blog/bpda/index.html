<!doctype html>
<html lang="en">
  <head>
  <meta charset="utf-8">
<title>Bayesian divergence point analysis of visual world data - Kate Stone | University of Hull</title>
<meta name="viewport" content="width=device-width, initial-scale=1">


  <link rel="apple-touch-icon" sizes="180x180" href="/favicon/apple-touch-icon.png?v=1">
  <link rel="icon" type="image/png" sizes="32x32" href="/favicon/favicon-32x32.png?v=1">
  <link rel="icon" type="image/png" sizes="16x16" href="/favicon/favicon-16x16.png?v=1">
  <link rel="manifest" href="/favicon/site.webmanifest?v=1">
  
    <link rel="mask-icon" href="/favicon/safari-pinned-tab.svg?v=1" color="#ffffff">
    <link rel="shortcut icon" href="/favicon/favicon.ico?v=1">
    <meta name="msapplication-config" content="/favicon/browserconfig.xml?v=1">
  
  <meta name="msapplication-TileColor" content="#ffffff">
  <meta name="theme-color" content="#ffffff">

<meta name="generator" content="Hugo 0.80.0" /><meta property="og:site_name" content="Kate Stone | University of Hull">
  <meta property="og:title" content="Bayesian divergence point analysis of visual world data">
  <meta property="og:description" content="A theme by HTML5 UP, ported by Julio Pescador. Slimmed and enhanced by Patrick Collins. Multilingual by StatnMap. Powered by Hugo.">
  <meta property="description" content="A theme by HTML5 UP, ported by Julio Pescador. Slimmed and enhanced by Patrick Collins. Multilingual by StatnMap. Powered by Hugo.">
  <meta property="og:url" content="/blog/bpda/">
  <meta property="og:type" content="article">
  
    <meta property="og:image" content="/images/headshot2.jpg">
  
  <link rel="stylesheet" href="/css/bundle.min.b3b530d2f6ca13612b2daa92175d4ffb0314aca099d7247c1a44af03355398e8.css" integrity="sha256-s7Uw0vbKE2ErLaqSF11P&#43;wMUrKCZ1yR8GkSvAzVTmOg="><link rel="stylesheet" href="/css/add-on.css">
</head>

  <body>
    

<header id="site-header">
  <nav id="site-nav">
    <h1 class="nav-title">
      <a href="/" class="nav">
        
          Kate Stone
        
      </a>
    </h1>
    <menu id="site-nav-menu" class="flyout-menu menu">
      
        
          
          <a href="/" class="nav link"> Home</a>
        
      
        
          
          <a href="/blog/" class="nav link"> Blog</a>
        
      
        
          
          <a href="/publications/" class="nav link"> Publications</a>
        
      
        
          
          <a href="/forparticipants/" class="nav link"> For participants</a>
        
      
      
      <a href="#search-input" class="nav link search-toggle"><i class="fas fa-search">&nbsp;</i>Search</a>
    </menu>
    <a href="#search-input" class="nav search-toggle"><i class="fas fa-search fa-2x">&nbsp;</i></a>
    
    
    <a href="#site-nav" class="nav nav-toggle"><i class="fas fa-bars fa-2x"></i></a>
  </nav>
  <menu id="search" class="menu"><input id="search-input" class="search-input menu"></input><div id="search-results" class="search-results menu"></div></menu>
  
  
</header>

    <div id="wrapper">
      <section id="site-intro" >
  <a href="/"><img src="/images/headshot2.jpg" class="circle" width="100" alt="photo" /></a>
  <header>
    <h1>Kate Stone</h1>
  </header>
  <main>
    <p>Lecturer | University of Hull</p>
  </main>
  
    <footer>
      <ul class="socnet-icons">
        
        


































<li><a href="//scholar.google.com/citations?user=https%3a%2f%2fscholar.google.com%2fcitations%3fuser%3dJQpDw6wAAAAJ%26hl%3den" target="_blank" rel="noopener" title="Google Scholar"><i class="ai ai-google-scholar"></i></a></li>
<li><a href="//orcid.org/0000-0002-2180-9736" target="_blank" rel="noopener" title="ORCID"><i class="ai ai-orcid"></i></a></li>



<li><a href="mailto:k.stone@hull.ac.uk" target="_blank" title="Email" class="far fa-envelope"></a></li>

      </ul>
    </footer>
  
</section>

      <main id="site-main">
        
  <article class="post">
    <header>
  <div class="title">
    
      <h2><a href="/blog/bpda/">Bayesian divergence point analysis of visual world data</a></h2>
    
    
  </div>
  <div class="meta">
    <time datetime="2021-04-08 00:00:00 &#43;0000 UTC">
	April 8, 2021
</time>
    <p>Kate Stone</p>
    
  </div>
</header>

    <div id="socnet-share">
      





    </div>
    <div class="content">
      
      
<script src="/rmarkdown-libs/header-attrs/header-attrs.js"></script>


<p>The visual world is a useful paradigm in psycholinguistics for tracking people’s eye fixations as they listen to sentences containing some kind of experimental manipulation. A common question is whether the experimental manipulation makes people look at a target object earlier in one condition than another. To answer this, we need to decide <em>when</em> in each condition people start looking at the target and compare these timepoints between conditions. But this is not as straightforward as it sounds!<a href="#fn1" class="footnote-ref" id="fnref1"><sup>1</sup></a> We came up with a bootstrapping method to do this in <a href="https://doi.org/10.1017/S1366728920000607">Stone, Lago &amp; Schad, 2020</a>. However, bootstrapping makes some unlikely assumptions about data—I’ll come to these later—and so here we try to improve on these assumptions by adding Bayesian principles to our method. This was an approach developed with <a href="https://www.jverissimo.net/">João Veríssimo</a>, <a href="https://danielschad.github.io/">Daniel Schad</a>, and <a href="https://sollago.github.io/">Sol Lago</a>, and we apply it in <a href="https://osf.io/3uz7x/">this paper</a>.</p>
<div id="the-bootstrap-procedure" class="section level3">
<h3>The bootstrap procedure</h3>
<p>A fully coded tutorial of the bootstrap procedure in Stone et al. (2020) is at <a href="https://osf.io/exbmk/">https://osf.io/exbmk/</a>. For a quick overview here, we use example data from an experiment on using syntactic gender to predict an upcoming noun in German. Participants heard sentences like “<em>Klicke auf seinen blauen…</em>” (click on his.<sub>MASC</sub> blue.<sub>MASC</sub> …), while looking at two blue objects on a screen. Only one of the objects matched the gender marking of the pronoun and adjective. There were two experimental conditions: in the <code>match</code> condition, the target object and the object’s owner matched in gender, e.g. <em>seinen Knopf</em> (his button.<sub>MASC</sub>). In the <code>mismatch</code> condition, the target object and the object’s owner mismatched in gender, e.g. <em>ihren Knopf</em> (her button.<sub>MASC</sub>).</p>
<center>
<img src="/images/objects.PNG" style="width:85.0%;height:85.0%" />
</center>
<!-- ```{r, fig.align="center"} -->
<!-- knitr::include_graphics("/images/objects.PNG") -->
<!-- ``` -->
<p>Because of the gender cue on the possessive and adjective (e.g. -en suffix), we expected participants to predict the target object and look preferentially at it before they heard its name. What we really wanted to know though was whether predictive looks would be delayed in the mismatch condition where there was a conflicting gender cue, even though the conflicting cue was syntactically irrelevant (i.e. the object’s owner being masculine or feminine has no bearing on what the upcoming object might be). You can already see below that the onset of when participants looked preferentially at the target appears to be later in the mismatch condition:</p>
<p><img src="/blog/bpda_files/figure-html/unnamed-chunk-1-1.png" width="576" style="display: block; margin: auto;" /></p>
<p>But is this “prediction onset” in the mismatch condition <em>significantly</em> later than the match condition? We can find out using our bootstrapping procedure, which has the following steps:</p>
<ol style="list-style-type: decimal">
<li>Conduct a statistical test of fixations between the target and competitor at each timepoint in each condition (similar to a permutation test, e.g. <a href="https://doi.org/10.1111/j.1469-8986.2011.01273.x">Groppe, Urbach &amp; Kutas, 2011</a>; <a href="https://doi.org/10.1016/j.jneumeth.2007.03.024">Maris &amp; Oostenveldt, 2007</a>; <a href="https://doi.org/10.1037/a0031813">Barr, Jackson &amp; Phillips, 2014</a>),</li>
<li>Decide on an alpha (usually 0.05) and find the first significant test statistic in a run of five consecutive significant test statistics (<a href="https://doi.org/10.1080/13506285.2012.693548">Sheridan &amp; Reingold, 2012</a>; <a href="https://doi.org/10.3389/fpsyg.2014.01432">Reingold &amp; Sheridan, 2014</a> take a similar approach).<a href="#fn2" class="footnote-ref" id="fnref2"><sup>2</sup></a> This was our divergence point for each condition, which we consider the onset of evidence for predictions,</li>
<li>Resample the data 2000 times with replacement and repeat steps 1-2 after each resample.</li>
</ol>
<p>The procedure thus has 3 distinct components:</p>
<ol style="list-style-type: lower-roman">
<li>a set of statistical tests comparing fixations to the target vs. competitor,</li>
<li>a criterion for deciding where the onset is, and</li>
<li>a way to generate a distribution of these onsets (resampling).</li>
</ol>
<p>The unique contribution of our procedure versus existing methods was iii): we estimate the sampling distribution of an onset in each condition, which we can then use to statistically compare onsets between conditions.</p>
<p>The procedure yields two bootstrap distributions: one distribution each of onsets for the match and mismatch conditions. We take the mean and the 95th percentile confidence interval (CI) of the match/mismatch distributions as an estimate of the prediction onset for each condition and its temporal variability:</p>
<p><img src="/blog/bpda_files/figure-html/unnamed-chunk-2-1.png" width="672" style="display: block; margin: auto;" /></p>
<p>By subtracting the match from the mismatch distribution, we obtain a distribution of differences between conditions. We take the mean and the 95th percentile CI of this difference distribution as the estimated difference in prediction onset. We can decide whether the difference is different from zero by looking at whether the 95th percentile CI of the difference distribution contains zero. Since it does not, we can conclude that the difference between conditions is not zero. Moreover, since all values in the distribution are positive, we can conclude that predictions were slower in the mismatch condition:</p>
<p><img src="/blog/bpda_files/figure-html/unnamed-chunk-3-1.png" width="384" style="display: block; margin: auto;" /></p>
<hr />
<div id="limitations-of-the-bootstrap-procedure" class="section level4">
<h4>Limitations of the bootstrap procedure</h4>
<p>As mentioned earlier, the bootstrap makes some unlikely assumptions (<a href="http://www.sumsar.net/blog/2015/04/the-non-parametric-bootstrap-as-a-bayesian-model/">Bååth, 2018</a>):</p>
<ul>
<li>Any values not seen in the observed data are impossible</li>
<li>Each value in the observed data has an equal probability of occurring every time the experiment is run</li>
<li>We have no prior knowledge about what datapoints might be observed</li>
</ul>
<p>These assumptions limit our interpretation of the results. Another limitation of our procedure above is that while it allows us to conclude that there is a significant difference between match and mismatch prediction onsets, it does not allow us to quantify <em>how much</em> evidence we have for this conclusion.</p>
<hr />
</div>
</div>
<div id="adding-bayesian-principles" class="section level3">
<h3>Adding Bayesian principles</h3>
<p>Bayesian inference estimates our certainty about an event based on our prior knowledge about the probability of that event and new data. In our case, the event is prediction onsets. We can estimate our certainty about prediction onsets via Bayes’ theorem, which estimates a <a href="https://en.wikipedia.org/wiki/Posterior_probability">posterior probability distribution</a> using two components: priors to encode our expectations about when the onset could be, and data to inform posterior inference via a <a href="https://en.wikipedia.org/wiki/Likelihood_function">likelihood function</a><a href="#fn3" class="footnote-ref" id="fnref3"><sup>3</sup></a>.</p>
<p>We start with the priors: we reasoned that prediction onsets could only arise in the 1600 ms time window between the onset of the pronoun and the onset of the noun (adding 200 ms for saccade planning). We therefore specified a normal distribution centered in the middle of this critical window, with a 95% probability of the onset falling between 200 and 1800 ms: <span class="math inline">\(N(1000,400)\)</span>:</p>
<pre class="r"><code>prior_mean  &lt;- 1000
prior_sd    &lt;- 400</code></pre>
<p>Our prior distributions for the match and mismatch conditions therefore looked like this:</p>
<p><img src="/blog/bpda_files/figure-html/unnamed-chunk-5-1.png" width="768" style="display: block; margin: auto;" /></p>
<p>Next, we needed a likelihood function. This involved two steps: We used the bootstrap data to approximate a likelihood and a normal distribution to approximate a likelihood function (i.e. Laplace approximation). This was based on the assumption of the central limit theorem that the population distribution underlying the bootstrap data was approximately normal<a href="#fn4" class="footnote-ref" id="fnref4"><sup>4</sup></a>. In other words, our likelihood function was a normal distribution with the mean and standard deviation of the bootstrap data:</p>
<pre class="r"><code># match condition
likelihood_mean_match    &lt;- mean(bootstrap_samples$match, na.rm = TRUE)
likelihood_sd_match      &lt;- sd(bootstrap_samples$match, na.rm = TRUE)*
                                 (length(bootstrap_samples$match)-1)/length(bootstrap_samples$match)


# mismatch condition
likelihood_mean_mismatch &lt;- mean(bootstrap_samples$mismatch, na.rm = TRUE)
likelihood_sd_mismatch   &lt;- sd(bootstrap_samples$mismatch, na.rm = TRUE)*
                                 (length(bootstrap_samples$mismatch)-1)/length(bootstrap_samples$mismatch)</code></pre>
<p><img src="/blog/bpda_files/figure-html/unnamed-chunk-7-1.png" width="768" style="display: block; margin: auto;" /></p>
<p>Finally, we could now derive the posterior distribution as the product of the prior and the likelihood. Because the prior and the likelihood are normal distributions, the posteriors for each condition can be derived analytically as the <a href="https://ccrma.stanford.edu/~jos/sasp/Product_Two_Gaussian_PDFs.html">product of two Gaussian probability density functions</a>.</p>
<ul>
<li><span class="math inline">\(N(\mu_{prior},\sigma_{prior})\)</span> is the prior,</li>
<li><span class="math inline">\(N(\mu_{lik},\sigma_{lik})\)</span> is the likelihood,</li>
<li><span class="math inline">\(\mu_{posterior}\)</span> is the mean of the posterior, and</li>
<li><span class="math inline">\(\sigma_{posterior}\)</span> is the standard deviation of <span class="math inline">\(\mu_{posterior}\)</span></li>
</ul>
<p>The posterior distribution of the onset in the match condition is therefore:</p>
<p><span class="math display">\[ \begin{aligned}
\mu_{posterior} &amp;= \frac{\mu_{prior} \sigma_{lik}^2 + \mu_{lik} \sigma_{prior}^2}{\sigma_{lik}^2 + \sigma_{prior}^2} \\
&amp;= \frac{1000 \cdot 22^2 + 367 \cdot 400^2}{22^2 + 400^2} \\ &amp;= 369 ms
\end{aligned}
\]</span></p>
<p><span class="math display">\[ \begin{aligned}
\sigma_{posterior} &amp;= \sqrt\frac{\sigma_{prior}^2 \sigma_{lik}^2}{\sigma_{prior}^2 + \sigma_{lik}^2} \\ &amp;= 
\sqrt\frac{400^2 \cdot 22^2}{400^2 + 22^2} \\ &amp;= 22 ms 
\end{aligned}
\]</span>
Via the same calculation, the posterior for the mismatch onset is 705 ms (SD = 9 ms):</p>
<pre class="r"><code># match
posterior_mean_match     &lt;- ((likelihood_mean_match*prior_sd^2) + (prior_mean*likelihood_sd_match^2)) /
                                (prior_sd^2 + likelihood_sd_match^2)
posterior_sd_match       &lt;- sqrt( (likelihood_sd_match^2 * prior_sd^2) / (likelihood_sd_match^2 + prior_sd^2) )


# mismatch
posterior_mean_mismatch  &lt;- ((likelihood_mean_mismatch*prior_sd^2) + (prior_mean*likelihood_sd_mismatch^2)) /
                                (prior_sd^2 + likelihood_sd_mismatch^2)
posterior_sd_mismatch    &lt;- sqrt( (likelihood_sd_mismatch^2 * prior_sd^2) / (likelihood_sd_mismatch^2 + prior_sd^2) )</code></pre>
<p>We can add these posteriors to our plots and see that they resemble the bootstrap data:</p>
<p><img src="/blog/bpda_files/figure-html/unnamed-chunk-9-1.png" width="768" style="display: block; margin: auto;" /></p>
<p>This is because our prior was relatively uninformative, and so the posteriors are informed more strongly by the bootstrap data than by the prior. If we had defined a strongly informative prior to say we were very certain that the prediction onsets would be 1000 ms, e.g. <span class="math inline">\(N(1000, 10)\)</span>, then the posteriors would be pulled toward the prior (not completely, as they’re still being informed by the data):</p>
<p><img src="/blog/bpda_files/figure-html/unnamed-chunk-10-1.png" width="960" style="display: block; margin: auto;" /></p>
<p>How do the Bayesian onsets (red, uninformative priors) compare with the onsets from the original bootstrap procedure (black)? Quite well:</p>
<p><img src="/blog/bpda_files/figure-html/unnamed-chunk-11-1.png" width="576" style="display: block; margin: auto;" /></p>
<p>Now, how to decide whether the mismatch condition was slower? Because the match and mismatch posteriors are normal distributions, we can find the posterior of their difference as the <a href="https://mathworld.wolfram.com/NormalDifferenceDistribution.html">difference of two normal distributions</a>:</p>
<p><span class="math display">\[ \begin{aligned}
\mu_{posterior_{difference}} &amp;= \mu_{posterior_{mismatch}} - \mu_{posterior_{match}} \\
 &amp;= 705 - 369 \\
 &amp;= 336 ms
\end{aligned}
\]</span></p>
<p><span class="math display">\[ \begin{aligned}
\sigma_{posterior_{difference}} &amp;= \sqrt(\sigma^2_{posterior_{mismatch}} + \sigma^2_{posterior_{match}}) \\
 &amp;= \sqrt(9^2 + 22^2) \\
 &amp;= 24ms
\end{aligned}
\]</span></p>
<pre class="r"><code>posterior_mean_difference  &lt;- posterior_mean_mismatch - posterior_mean_match
posterior_sd_difference    &lt;- sqrt( posterior_sd_mismatch^2 + posterior_sd_match^2 )</code></pre>
<p>We now have a posterior estimate that predictions in the mismatch condition were 336 ms slower than in the match condition, with a 95% credible interval of 288–384 ms. This posterior aligns quite well with the original bootstrap difference distribution, which estimated a difference of 338 ms with a 95th percentile interval of 300–380ms.</p>
<div id="quantifying-evidence-with-a-bayes-factor" class="section level4">
<h4>Quantifying evidence with a Bayes factor</h4>
<p>How much has seeing the data changed our belief in the null hypothesis that the difference between match/mismatch conditions is zero? Since the likelihood is not normalised, we can use the Savage-Dickey method to compute a Bayes factor and quantify evidence against the null (<a href="https://www.jstor.org/stable/2239734">Dickey &amp; Lientz, 1970</a>; <a href="https://doi.org/10.1016/j.cogpsych.2009.12.001">Wagenmakers et al., 2010</a>). This method finds the ratio of prior to posterior density at some point value (e.g. zero):</p>
<p><img src="/blog/bpda_files/figure-html/unnamed-chunk-13-1.png" width="336" style="display: block; margin: auto;" /></p>
<p>We compute the ratio via the equation below, where</p>
<ul>
<li><span class="math inline">\(\theta\)</span> is the point at which we want to compare densities (e.g. zero),</li>
<li><span class="math inline">\(H_0\)</span> is our prior distribution,</li>
<li><span class="math inline">\(H_1\)</span> is our posterior distribution,</li>
<li>and <span class="math inline">\(D\)</span> is the data:</li>
</ul>
<p><span class="math display">\[ BF_{01} = \frac{p(D|H_1)}{p(D|H_0)}= \frac{p(\theta = 0|H_1)}{p(\theta = 0|D,H_1)} \]</span></p>
<p>Or in R form:</p>
<pre class="r"><code># define the null hypothesis
null_hypothesis &lt;- 0

# define a prior for the distribution of differences 
prior_mean_difference   &lt;- prior_mean - prior_mean
prior_sd_difference     &lt;- sqrt(prior_sd^2 + prior_sd^2)

# find the density of the prior at the null hypothesis
density_prior_null     &lt;- dnorm(null_hypothesis, 
                                prior_mean_difference, 
                                prior_sd_difference)

# find the density of the posterior at the null hypothesis
density_posterior_null &lt;- dnorm(null_hypothesis, 
                                posterior_mean_difference, 
                                posterior_sd_difference)


# use Savage-Dickey equation to compute the Bayes factor
(BF01 &lt;- density_posterior_null/density_prior_null)</code></pre>
<pre><code>## [1] 1.402512e-44</code></pre>
<p>The Bayes factor can be interpreted as a ratio of evidence for one hypothesis over the other. Because our posterior density at zero is less than the prior density (in fact, the posterior density at zero is almost zero), the Bayes factor is less than 1 and thus favours the alternative hypothesis that there is a match/mismatch difference. We can see more clearly how overwhelminlgy it favours the alternative hypothesis if we flip the ratio to test the alternative vs. null hypotheses (BF10) rather than the null vs. alternative (BF01):</p>
<pre class="r"><code>(BF10 &lt;- 1/BF01)</code></pre>
<pre><code>## [1] 7.130063e+43</code></pre>
</div>
</div>
<div id="conclusions" class="section level3">
<h3>Conclusions</h3>
<p>Using our Bayesian divergence point analysis, we find strong evidence that prediction onsets were slower when there were two conflicting gender cues. The posterior estimate of the “mismatch effect” size was 336 ms, with a 95% credible interval of 288–384 ms. But our existing bootstrap procedure already led us to the same conclusion, so what was the advantage of adding Bayesian principles?</p>
<p>The Bayesian approach has all the advantages that Bayesian inference has over frequentist null hypothesis significance testing. These include that the posterior is a continuous probability distribution that gives us information about the probability of non-observed values in the data. The 95% credible interval can thus be interpreted as a range of possible between-condition differences in which the true size of the difference should lie with 95% probability, given the data and the analysis. This interpretation is more intuitive than the percentile confidence interval from the existing bootstrap method, which just told us were 95% of the resampled data lay. Moreover, we can now quantify evidence for our conclusions, and even for the null hypothesis, using a Bayes factor. Finally, because the Bayesian method uses priors, we can use our posterior estimates as information about plausible effect sizes to inform the priors of future experiments.</p>
<!-- ### Appendix 1 {-} -->
<!--  Above we mentioned that the distribution of our bootstrap data is not normal, and so using a normal distribution as a likelihood may not be appropriate. Instead, we can use a density function as our likelihood, although this means it is no longer possible to obtain the posterior as the product of two Gaussians. Also, our likelihood is still not normalised. To solve these issues, here we demonstrate how to use a density likelihood and obtain the posterior via sampling.  -->
<!-- First we define a likelihood for the match condition using a kernel density estimator of the original bootstrap data: -->
<!-- ```{r, echo=TRUE} -->
<!-- # define a range of onsets for which we would like to find onset probabilities, -->
<!-- # roughly corresponding to our window of interest: -->
<!-- values <- seq(0, 1900, 1) -->
<!-- # kernel density estimator: -->
<!-- density_likelihood_match  <- data.frame(dens = density(bootstrap_samples$match,  -->
<!--                                                     from = min(values), -->
<!--                                                     to = max(values), -->
<!--                                                     n = length(values))$y) -->
<!-- ``` -->
<!-- Then we need a density distribution for the prior using the prior mean and SD we used above: -->
<!-- ```{r, echo=TRUE} -->
<!-- density_prior_match <- data.frame(dens = dnorm(values,  -->
<!--                                                mean = prior_mean,  -->
<!--                                                sd = prior_sd)) -->
<!-- ``` -->
<!-- Then we multiply the likelihood and prior density distributions to get the posterior density distribution: -->
<!-- ```{r, echo=TRUE} -->
<!-- density_posterior_match <- data.frame(post = density_likelihood_match$dens *  -->
<!--                                         density_prior_match$dens) -->
<!-- ``` -->
<!-- And finally we sample from the posterior to obtain a *probability* distribution of onsets: -->
<!-- ```{r, echo=TRUE} -->
<!-- samples_posterior_match <- data.frame(post = sample(values, size=100000, -->
<!--                                     replace = TRUE, -->
<!--                                     prob = density_posterior_match$post)) -->
<!-- ``` -->
<!-- The parameters of our sampled posterior are: -->
<!-- ```{r, echo=TRUE} -->
<!-- # mean -->
<!-- round(mean(samples_posterior_match$post)) -->
<!-- # 95% CrI -->
<!-- quantile(samples_posterior_match$post, probs = c(.025, .975)) -->
<!-- ``` -->
<!-- We repeat for the  mismatch condition and get: -->
<!-- ```{r} -->
<!-- # define a likelihood using kernel density estimator -->
<!-- density_likelihood_mismatch   <- data.frame(dens = density(bootstrap_samples$mismatch,  -->
<!--                                                          from = min(values), -->
<!--                                                          to = max(values), -->
<!--                                                          n = length(values))$y) -->
<!-- # sample from the posterior using the density likelihood -->
<!-- ## create a density distribution of the prior -->
<!-- density_prior_mismatch        <- data.frame(dens = dnorm(values,  -->
<!--                                                          mean = prior_mean,  -->
<!--                                                          sd = prior_sd)) -->
<!-- ## then multiply the likelihood and prior PDFs -->
<!-- density_posterior_mismatch    <- data.frame(post = density_likelihood_mismatch$dens *  -->
<!--                                               density_prior_mismatch$dens) -->
<!-- ## then sample from the posterior -->
<!-- samples_posterior_mismatch    <- data.frame(post = sample(values, size = 100000, -->
<!--                                                     replace = TRUE, -->
<!--                                                     prob = density_posterior_mismatch$post)) -->
<!-- ``` -->
<!-- ```{r, echo=TRUE} -->
<!-- # mean -->
<!-- round(mean(samples_posterior_mismatch$post)) -->
<!-- # 95% CrI -->
<!-- quantile(samples_posterior_mismatch$post, probs = c(.025, 0.975)) -->
<!-- ``` -->
<!-- If we overlay these on our fixation curves, we can see that the sampling onset posteriors (purple) yield similar estimates to the normal distribution onset posteriors (red). This suggests that the normal distribution approach actually did a pretty good job approximating the posteriors, even though the bootstrap data didn't look very normal: -->
<!-- ```{r, fig.height=5, fig.width=10} -->
<!-- # match -->
<!-- posterior_mean_ma_norm <- mean(samples_posterior_match$post) -->
<!-- posterior_ci.lower_ma_norm <- quantile(samples_posterior_match$post, probs = c(.025))[[1]] -->
<!-- posterior_ci.upper_ma_norm <- quantile(samples_posterior_match$post, probs = c(.975))[[1]] -->
<!-- # mismatch -->
<!-- posterior_mean_mi_norm <- mean(samples_posterior_mismatch$post) -->
<!-- posterior_ci.lower_mi_norm <- quantile(samples_posterior_mismatch$post, probs = c(.025))[[1]] -->
<!-- posterior_ci.upper_mi_norm <- quantile(samples_posterior_mismatch$post, probs = c(.975))[[1]] -->
<!-- ### Plot -->
<!-- ggplot(plot_exp2, aes(Time, MeanFixation)) + -->
<!--   stat_summary(fun.data = mean_cl_boot,  -->
<!--                aes(fill = Region), geom = "ribbon", alpha = .25, show.legend = FALSE) + -->
<!--   stat_summary(fun = mean, geom = "path",  -->
<!--                aes(group = Region, colour = Region, linetype = Region), size = 1) + -->
<!--   facet_grid(.~ Condition, labeller = as_labeller(facet_names)) + -->
<!--   # add vertical lines for time windows -->
<!--   geom_hline(yintercept = .5, linetype = "dotted") + -->
<!--   geom_vline(xintercept = onsets, linetype = "dashed", colour = "grey50") + -->
<!--   # add word stimuli -->
<!--   geom_text(data = subset(words, Condition == "match"), label = "seinen", y = .13,  -->
<!--             x = 0, angle = 90, fontface = "italic", size = 5, colour = "grey50") + -->
<!--   geom_text(data = subset(words, Condition == "mismatch"), label = "ihren", y = .12,  -->
<!--             x = 0, angle = 90, fontface = "italic", size = 5, colour = "grey50") + -->
<!--   annotate(geom = "text", label = "blauen", x = onsets[1]-60, y = .13, angle = 90, -->
<!--            fontface = "italic", size = 5, colour = "grey50") + -->
<!--   annotate(geom = "text", label = "Knopf", x = onsets[2]-60, y = .11, angle = 90, -->
<!--            fontface = "italic", size = 5, colour = "grey50") + -->
<!--   # add bootstrapped onset and credible interval in the MATCH condition -->
<!--   ## normal likelihood method -->
<!--   geom_point(data = filter(plot_exp2, Condition == "match"),  -->
<!--              size = 4, colour = "red2", aes(x = posterior_mean_ma, y = .55)) + -->
<!--   geom_errorbarh(data = filter(plot_exp2, Condition == "match"), -->
<!--                  height = .07, colour = "red2", aes(y    = .55, -->
<!--                   xmin = qnorm(c(.025), posterior_mean_ma, posterior_sd_ma), -->
<!--                   xmax = qnorm(c(.975), posterior_mean_ma, posterior_sd_ma))) + -->
<!--   ## density likelihood method -->
<!--   geom_point(data = filter(plot_exp2, Condition == "match"), -->
<!--              size = 4, colour = "purple", aes(x = posterior_mean_ma_norm, y = .45)) + -->
<!--   geom_errorbarh(data = filter(plot_exp2, Condition == "match"), -->
<!--                  size = 1, height = .07, colour = "purple", -->
<!--                  aes(y    = .45, -->
<!--                      xmin = posterior_ci.lower_ma_norm, -->
<!--                      xmax = posterior_ci.upper_ma_norm)) + -->
<!--   # add bootstrapped onset and credible interval in the MISMATCH condition -->
<!--   ## normal likelihood method -->
<!--   geom_point(data = filter(plot_exp2, Condition == "mismatch"), -->
<!--              size = 4, colour = "red2", aes(x = posterior_mean_mi, y = .55)) + -->
<!--   geom_errorbarh(data = filter(plot_exp2, Condition == "mismatch"), -->
<!--                  size = 1, height = .07, colour = "red2", aes(y = .55, -->
<!--                     xmin = qnorm(c(.025), posterior_mean_mi, posterior_sd_mi), -->
<!--                     xmax = qnorm(c(.975), posterior_mean_mi, posterior_sd_mi))) + -->
<!--   ## density likelihood method -->
<!--   geom_point(data = filter(plot_exp2, Condition == "mismatch"), -->
<!--              size = 4, colour = "purple", aes(x = posterior_mean_mi_norm, y = .45)) + -->
<!--   geom_errorbarh(data = filter(plot_exp2, Condition == "mismatch"), -->
<!--                  height = .07, colour = "purple", -->
<!--                  aes(y    = .45, -->
<!--                      xmin = posterior_ci.lower_mi_norm, -->
<!--                      xmax = posterior_ci.upper_mi_norm)) + -->
<!--   # axes and legends -->
<!--   labs(x = "Time since possessive onset [ms]", y = "Fixations to objects") + -->
<!--   scale_x_continuous(breaks = seq(0, 2500, 500), labels = seq(0, 2500, 500)) + -->
<!--   scale_y_continuous(labels = scales::percent_format(accuracy = 1)) + -->
<!--   scale_linetype_manual(values = lty, name = "Object:") + -->
<!--   scale_colour_manual(values = col, name = "Object:") + -->
<!--   scale_fill_manual(values = col, name = "Object:") + -->
<!--   # theme settings -->
<!--   theme_light() + -->
<!--   theme(text = element_text(size = 20),  -->
<!--         legend.position = "none", -->
<!--         strip.text = element_text(size = 20)) -->
<!-- ``` -->
<!-- ```{r, eval=FALSE, out.height="85%", out.width="75%", fig.align="center", fig.cap="\\label{fig:sampledlikelihood}Comparison of a posterior obtained as the product of two Gaussians (red) vs. via sampling (blue)."} -->
<!-- # posterior as multiplication of prior and likelihood pdfs, followed by sampling -->
<!-- plot(density(na.omit(bootstrap_samples$dp)), xlim=xlims, col = "grey50",  -->
<!--      xlab = "Match-mismatch onset difference (ms)",  -->
<!--      main = "Comparison of posterior methods", frame.plot = FALSE, yaxt = "n") -->
<!-- axis(side = 2, labels = FALSE) -->
<!-- lines(prior.df$dens ~ values, xlim=xlims, lty = 1, lwd = 2) -->
<!-- lines(likelihood.df$dens ~ values, lty = 2, xlim=xlims, lwd = 2) -->
<!-- abline(v=posterior.mean.sampled) -->
<!-- # lines(dnorm(0:600, posterior.mean.sampled, posterior.sd.sampled), xlim=xlims, col = "red2", lwd = 2, lty=1) -->
<!-- lines(likelihood$dens ~ values, lty = 2, xlim=xlims, lwd = 2, col="red2") -->
<!-- lines(density(posterior.samples$post), xlim=xlims, col = "blue4", lwd = 2, lty=1) -->
<!-- legend("topleft", bty = "n", col = c("grey50","black","black","red2","blue4"),  -->
<!--        legend = c("bootstrap data","prior","likelihood: density","likelihood: normal","posterior: sampled"), -->
<!--        lty = c(1,1,2,2,1), lwd = c(2,2,2,2,2)) -->
<!-- ``` -->
</div>
<div class="footnotes">
<hr />
<ol>
<li id="fn1"><p>There are several existing methods for answering temporal questions about visual world data, including <a href="https://doi.org/10.1037/a0031813">cluster permutation</a>, <a href="https://doi.org/10.1016/j.jml.2018.05.004">BDOTS</a>, and GAMMs. We summarise these in <a href="https://doi.org/10.1017/S1366728920000607">Stone et al., 2020</a> and outline why they weren’t able to answer our specific question about whether one onset was significantly faster than another.<a href="#fnref1" class="footnote-back">↩︎</a></p></li>
<li id="fn2"><p>Depending on your experimental manipulation (e.g. how big or sustained you expect your experimental effect to be) and how you’ve set up your data (e.g. binned, unbinned, eye tracker sampling rate), your criterion for the number of consecutive significant tests may differ.<a href="#fnref2" class="footnote-back">↩︎</a></p></li>
<li id="fn3"><p>Other possibilities for adding Bayesian principles to our procedure could have been to use the Bayesian Bootstrap (<a href="https://doi.org/10.1214/aos/1176345338">Rubin, 1981</a>), implemented in R in <code>bayesboot</code> <a href="http://www.sumsar.net/blog/2016/02/bayesboot-an-r-package/">(Bååth, 2018)</a>. Unfortunately, <code>bayesboot</code> didn’t suit our particular bootstrapping method, partly because it doesn’t take a multi-step function like ours (see steps 1-3 above), but also because it doesn’t take more informative priors—at least not currently. Alternatively, we could have fit e.g. GLMMs in <code>brms</code> <a href="https://github.com/paul-buerkner/brms">(Bürkner, 2018)</a> to test between fixation proportions at each timepoint (step 1). But this would only apply Bayesian inference to estimating the <em>magnitude</em> of the difference in fixations between target and competitor at each timepoint, when what we really want is to apply it to finding the temporal variability of the onset.<a href="#fnref3" class="footnote-back">↩︎</a></p></li>
<li id="fn4"><p>Using a normal distribution for the likelihood assumes that, with sufficient observations and bootstrap samples, the bootstrap distribution will approach a normal distribution in line with the central limit theorem. However, this is not always the case. An alternative way to define the likelihood would be to use a kernel density estimator instead: we present this approach in the appendices of <a href="https://osf.io/3uz7x/">this paper</a>.<a href="#fnref4" class="footnote-back">↩︎</a></p></li>
</ol>
</div>

    </div>
    <footer>
      <div class="stats">
  
  
  
  
</div>

    </footer>
  </article>
  
    

  
  <div class="pagination">
    
      <a href="/blog/opensesame/" class="button left"><span>Tutorial: An RSVP experiment in OpenSesame</span></a>
    
    
  </div>

      </main>
      <section id="site-sidebar">
  
    <section id="recent-posts">
      <header>
        <h1>Recent Posts</h1>
      </header>
      
      <article class="mini-post">
          
        <header>
          <h2><a href="/blog/opensesame/">Tutorial: An RSVP experiment in OpenSesame</a></h2>
          <time class="published" datetime="2025-03-18 00:00:00 &#43;0000 UTC">
	March 18, 2025
</time>
        </header>
      </article>
      
      <article class="mini-post">
          
        <header>
          <h2><a href="/blog/bpda/">Bayesian divergence point analysis of visual world data</a></h2>
          <time class="published" datetime="2021-04-08 00:00:00 &#43;0000 UTC">
	April 8, 2021
</time>
        </header>
      </article>
      
      
        <footer>
          <a href="//" class="button">See More</a>
        </footer>
      
    </section>
  

  

  
</section>

      <footer id="site-footer">
  
  
  
		<script src="/js/math-code.js"></script>
		<script async src="//cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML"></script>
  

  <p class="copyright">
    © 2025 Kate Stone | University of Hull
      <br>
    Theme: <a href='https://github.com/pacollins/hugo-future-imperfect-slim' target='_blank' rel='noopener'>Hugo Future Imperfect Slim</a><br>A <a href='https://html5up.net/future-imperfect' target='_blank' rel='noopener'>HTML5 UP port</a> | Powered by <a href='https://gohugo.io/' title='0.80.0' target='_blank' rel='noopener'>Hugo</a>
  </p>
</footer>
<a id="back-to-top" href="#" class="fas fa-arrow-up fa-2x"></a>

      <script src="/js/highlight.js"></script>
    
    <script>hljs.initHighlightingOnLoad();</script><script src="/js/bundle.min.b4e669fa428a81defb8af0916c53f39cd1b8e0bbab22199c06f0b182907ba474.js" integrity="sha256-tOZp&#43;kKKgd77ivCRbFPznNG44LurIhmcBvCxgpB7pHQ="></script>
    <script src="/js/add-on.js"></script>
    </div>
  </body>
</html>
