<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Blog on Kate Stone | University of Hull</title>
    <link>/blog/</link>
    <description>Recent content in Blog on Kate Stone | University of Hull</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en</language>
    <lastBuildDate>Fri, 11 Mar 2022 00:00:00 +0000</lastBuildDate><atom:link href="/blog/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Tutorial: An RSVP experiment in OpenSesame</title>
      <link>/blog/opensesame/</link>
      <pubDate>Fri, 11 Mar 2022 00:00:00 +0000</pubDate>
      
      <guid>/blog/opensesame/</guid>
      <description>OpenSesame is a free, open source tool for building experiments. In this tutorial, I’m going to build a rapid serial visual presentation (RSVP) paradigm of the kind typically used for presenting sentences word-by-word to participants in ERP experiments. The paradigm will send triggers at particular words to the EEG recording computer so that these words can later be located in the EEG recording.
PLEASE NOTE: The paradigm in this tutorial was designed using OpenSesame v3.</description>
    </item>
    
    <item>
      <title>Bayesian divergence point analysis of visual world data</title>
      <link>/blog/bpda/</link>
      <pubDate>Thu, 08 Apr 2021 00:00:00 +0000</pubDate>
      
      <guid>/blog/bpda/</guid>
      <description>The visual world is a useful paradigm in psycholinguistics for tracking people’s eye fixations as they listen to sentences containing some kind of experimental manipulation. A common question is whether the experimental manipulation makes people look at a target object earlier in one condition than another. To answer this, we need to decide when in each condition people start looking at the target and compare these timepoints between conditions.</description>
    </item>
    
  </channel>
</rss>
